# 1A

Lexical ambiguity is the potential for multiple interpretations of spoken or written language that renders it difficult or impossible to understand without some additional information.

Syntactic Ambiguity represents sentences that can be parsed in multiple syntactical forms.

|                  | Syntactic Ambiguity                            | Lexical Ambiguity                                        |
|------------------|------------------------------------------------|----------------------------------------------------------|
| Definition       | Arises due to the structure or arrangement of words in a sentence. | Arises due to the presence of multiple meanings for a single word. |
| Cause            | Ambiguity is caused by the sentence structure or grammar. | Ambiguity is caused by the multiple meanings of a single word. |
| Nature           | Involves different interpretations of the same sentence structure. | Involves different interpretations of a specific word within a sentence. |
| Resolution       | Resolved by analyzing the sentence structure or by considering the context. | Resolved by considering the context or through the use of additional information. |
| Example Sentence | "Time flies like an arrow; fruit flies like a banana." | "I saw a man on a hill with a telescope."                  |
| Explanation      | In the example, "Time flies" can be interpreted as "Time flies (insects) enjoy an arrow (like a bird)" or "Time flies (passes quickly) as an arrow (a metaphor for speed)." | In the example, "man on a hill with a telescope" can be interpreted as "a man standing on a hill who has a telescope" or "a man standing on a hill using a telescope." |
| Analysis         | The ambiguity is resolved by understanding the syntax and the relationships between the words in the sentence. | The ambiguity is resolved by considering the different meanings of the specific word in context or by using additional information. |
| Common Causes    | Sentence structure, word order, punctuation, and grammar can contribute to syntactic ambiguity. | Multiple meanings, homophones, homonyms, and polysemy contribute to lexical ambiguity. |
| Importance       | Important for understanding the intended meaning of a sentence and avoiding miscommunication. | Important for disambiguating the intended meaning of a specific word within a sentence. |


# 1B what are affixes, explain types of affixes

Affixes are morphemes that are added to the beginning (prefixes) or end (suffixes) of a base word to create a new word or modify the meaning or function of the base word. They play a crucial role in word formation and can change the grammatical category (part of speech) or alter the meaning of the base word. There are several types of affixes:

1. Prefixes: Prefixes are affixes that are added to the beginning of a base word. They can change the meaning or create a new word. Examples include:
   - Un- (unhappy, undo)
   - Dis- (dislike, disconnect)
   - Pre- (preview, preheat)

2. Suffixes: Suffixes are affixes that are added to the end of a base word. They can change the grammatical category or meaning of the word. Examples include:
   - -ly (quickly, beautifully)
   - -able (comfortable, understandable)
   - -er (teacher, runner)

3. Infixes: Infixes are affixes that are inserted into the middle of a base word. They are relatively rare in English but are more common in certain languages. Examples are limited in English, but one example is the expletive infix "bloody" (abso-bloody-lutely).

4. Circumfixes: Circumfixes are affixes that surround a base word, with one part added to the beginning and another part added to the end. They are also less common in English. Examples include:
   - En- + -ed (enlightened, enlivened)
   - Ge- + -ing (germinating, gesturing)

5. Interfixes: Interfixes are elements that occur between morphemes, but they do not change the meaning of the base word. They are relatively uncommon in English. For example, in the word "ham-burger," the interfix "-burg-" separates the base words "ham" and "-er."

6. Combining Forms: Combining forms are affixes used in the formation of compound words, often derived from Latin or Greek roots. They can be either prefixes or suffixes. Examples include:
   - Bio- (biology, biography)
   - Geo- (geology, geography)
   - Micro- (microscope, microbiology)

It's important to note that not all languages use affixes in the same way or to the same extent. The types and functions of affixes can vary across languages.

# 1C Describe openclass words and closedclass words in english with examples

In linguistics, words in a language are categorized into different classes based on their grammatical and semantic characteristics. One common way of classifying words is by dividing them into open class and closed class words.

1. Open Class Words:
Open class words, also known as content words, are words that have the ability to add new members and expand their category. They typically include nouns, verbs, adjectives, and adverbs. Open class words are relatively flexible and can easily accommodate new additions to the language. They tend to carry more meaning and play a central role in conveying information. Here are some examples:

- Nouns: dog, table, book
- Verbs: run, eat, speak
- Adjectives: happy, tall, blue
- Adverbs: quickly, carefully, very

2. Closed Class Words:
Closed class words, also known as function words, are words that have a limited number of members and do not readily accept new additions. They are considered more stable and less likely to change over time. Closed class words include pronouns, conjunctions, prepositions, and certain adverbs. They serve grammatical functions and provide structure to sentences. Examples of closed class words are:

- Pronouns: I, you, he, she
- Conjunctions: and, but, or
- Prepositions: in, on, at
- Articles: a, an, the

In Natural Language Processing (NLP), the concept of open class and closed class words is utilized to understand and process language. Open class words are often considered more important for tasks like information extraction, sentiment analysis, and text generation because they carry substantial semantic content. On the other hand, closed class words help establish the syntactic structure of sentences and aid in tasks such as parsing, part-of-speech tagging, and grammar checking.

NLP models and algorithms leverage the knowledge of open class and closed class words to analyze and process text. For instance, part-of-speech tagging algorithms assign appropriate tags to words in a sentence based on their grammatical role. This helps in understanding the syntactic structure and disambiguating word meanings. The distinction between open class and closed class words assists in designing language models and developing computational systems that can handle various linguistic tasks.



# 1D What is rule based machine translation ?

Rule-based machine translation (RBMT) is a traditional approach to machine translation in the field of Natural Language Processing (NLP). RBMT relies on a set of linguistic rules and patterns to translate text from one language to another.

In RBMT, the translation process is guided by a collection of handcrafted rules that encode language-specific patterns, grammar rules, syntactic structures, and lexical information. These rules are typically created by linguists and domain experts who analyze the source and target languages and define translation mappings based on their knowledge of the languages' structure and characteristics.

The RBMT system follows a sequential process to translate a sentence:

1. Analysis: The source sentence is first analyzed to identify its grammatical structure, parts of speech, and syntactic relationships. This step involves parsing the source sentence to understand its linguistic elements.

2. Transfer: Once the analysis is complete, the RBMT system applies a set of transformation rules that convert the analyzed structure and linguistic elements into the target language structure. These rules define how the source language elements should be mapped to their equivalents in the target language.

3. Generation: Finally, the RBMT system uses generation rules to create the translated sentence in the target language. These rules specify how the transformed structure and linguistic elements should be arranged to form a grammatically correct and fluent target sentence.

RBMT systems require a comprehensive set of rules that cover various linguistic phenomena and translation patterns. Linguistic expertise is crucial in building and maintaining such systems. The advantages of RBMT include explicit control over translation rules, which allows for precise linguistic analysis and fine-grained translation control.

However, RBMT also has limitations. It can be labor-intensive and time-consuming to develop and maintain the rules. Handling exceptions and capturing the nuances of different languages and contexts can be challenging. RBMT systems may struggle with translating idiomatic expressions, ambiguous phrases, or complex linguistic structures that require deeper semantic understanding.

As newer approaches such as statistical machine translation and neural machine translation have gained prominence in recent years, RBMT is less commonly used today. Nevertheless, RBMT still finds some applications, particularly for specific domains or low-resource languages where limited training data is available.

There are three types of computer-aided translation systems:

Rule-based machine translation (RBMT)

Statistical machine translation (SMT)

Neural machine translation (NMT)

Rule-Based Translation

Rule-based machine translation (RBMT) is based on programmed information that dictates how a word or phrase in the source language should read in the target language.

For example, an English word is added and the RBMT system outputs the best German word based on morphological, syntactic, and semantic analysis of both the source and the target languages involved in a translation task.

Here’s what we mean by those three deciding factors:

Morphological: What is the form or structure of the sentence?
Syntactic: What are the language rules that apply?
Semantic: What’s the basic meaning of the sentence?
RBMT requires a full vocabulary and language rules to function properly. So, to produce a German translation of this English sentence, you need:

a dictionary that will map each English word to an appropriate German word
rules representing regular English sentence structure
rules representing regular German sentence structure
Since language is dynamic and evolves over time, the efficacy of RBMT is limited and it certainly lives up to its “machine” translation moniker based on a lack of on-the-fly adaptability.

Statistical Machine Translation
Statistical machine translation (SMT) analyzes existing translations developed by humans (referred to as bilingual text corpora).

Whereas RBMT is a word-based approach, SMT systems are built on phrase-based systems. Instead of moving forward word by word, SMT can string them together into the likeliest phrase from bilingual text corpora.

Extensive analysis of bilingual text corpora (both the source and target languages) and monolingual corpora (the target language) generates statistical models that translate text from one language to another.

It’s basically what fueled the advent of online translation tools like Babel Fish Altavista and Google Translate, although the latter has since pivoted to neural machine translation.

Introduced in the mid-1990s, Babel Fish could automatically translate text into several languages. The program was freely available online and brought machine translation to the masses.

These programs made use of statistical machine translation, translating the source material based on the most common previous translations that have been previously done. They’re not without their flaws, though.

Like RBMT, SMT’s general weakness is that it can only translate a phrase if it exists in the reference texts.

Neural Machine Translation
Neural machine translation (NMT) differs from its rule- and stat-based precursors in having an ability to learn from each translation task and improve upon each subsequent translation.

NMT can recognize patterns in the source material to determine a context-based interpretation that can predict the likelihood of a sequence of words.

NMT is also known as deep learning. It’s a breakthrough because it does not have to be supervised by humans. It involves creating large neural networks that allow the system to learn and operate independently.

The linear logic used by traditional computers is replaced by a neural net method modelled on the human brain. The software learns and perfects after each new experience.



# 1E 

| Term       | Definition                                         | Examples                                                   |
|------------|----------------------------------------------------|------------------------------------------------------------|
| Homonymy   | Words that sound alike but have different meanings. | 1. Bank (financial institution) vs. bank (river edge)       |
|            |                                                    | 2. Bat (animal) vs. bat (sports equipment)                 |
|            |                                                    | 3. Bark (sound a dog makes) vs. bark (outer covering of a tree) |
| Polysemy   | Words that have multiple related meanings.          | 1. Book (physical object) vs. book (record a reservation)   |
|            |                                                    | 2. Run (physical activity) vs. run (manage a business)      |
|            |                                                    | 3. Light (brightness) vs. light (not heavy)                 |
| Synonymy   | Words that have similar meanings.                   | 1. Big vs. large                                           |
|            |                                                    | 2. Happy vs. joyful                                        |
|            |                                                    | 3. Fast vs. quick                                          |
| Antonymy   | Words that have opposite meanings.                  | 1. Hot vs. cold                                            |
|            |                                                    | 2. Tall vs. short                                          |
|            |                                                    | 3. Love vs. hate                                           |



# 1F Explain perplexity of any language model.

Perplexity is a metric used to evaluate the performance of a language model in predicting a sequence of words or tokens. It measures how well the model is able to assign probabilities to a given set of test data.

In the context of language modeling, perplexity quantifies how surprised a language model is when encountering a particular sequence of words. A lower perplexity indicates that the model is more certain about its predictions, while a higher perplexity indicates more uncertainty.

Perplexity is calculated using the following formula:

Perplexity = 2^H

where H is the average entropy of the model's predictions for each word in the test set. Entropy, in this case, represents the average number of bits required to encode the next word in the sequence according to the model's probability distribution.

To calculate the perplexity of a language model, you would typically follow these steps:

1. Preprocess your test data by tokenizing it into words or subword units.
2. Feed the preprocessed test data into the language model.
3. For each word or token in the test set, calculate the probability distribution provided by the model.
4. Compute the entropy for each word using the probability distribution. The entropy is calculated as the negative logarithm of the probability of the true word in the distribution.
5. Average the entropies of all the words in the test set.
6. Take the exponent of the average entropy to obtain the perplexity.

A lower perplexity indicates that the language model has a better understanding of the test data and can make more accurate predictions. Higher perplexity suggests that the model is less certain and may struggle with predicting the next word accurately.

Perplexity is often used as an evaluation metric during the training and fine-tuning of language models. It helps researchers and practitioners compare different models or variations of the same model and make informed decisions about their performance.
